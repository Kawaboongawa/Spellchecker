################################################################################
################################################################################

   _____            _ _  _____ _               _
  / ____|          | | |/ ____| |             | |
 | (___  _ __   ___| | | |    | |__   ___  ___| | _____ _ __
  \___ \| '_ \ / _ \ | | |    | '_ \ / _ \/ __| |/ / _ \ '__|
  ____) | |_) |  __/ | | |____| | | |  __/ (__|   <  __/ |
 |_____/| .__/ \___|_|_|\_____|_| |_|\___|\___|_|\_\___|_|
        | |
        |_|

################################################################################
################################################################################

VERSION :


                        Last Update done the 31/08/2017
                                 Spellchecker v1.0


AUTHORS :


                        LUGAND Jérémy lugand_j@epita.fr
                        CETRE Cyril   cyril.cetre@epita.fr


PREVIEW :


This is a C language spell checker building first a disk written
dictionary an then using it to give every word that matches
the request within a given distance of Damerau-Levenshtein.
This project was written for an EPITA school project.


REQUIREMENT :


This project is available on both MacOs and LINUX operating system.
You will only need a version of gcc and gcc-7 on MacOs.


BUILD :


To build the project run the make command
just as shown in the following in the root directory of the project :

42sh$ make

This should generate two different binaries : TextMiningCompiler and
TextMiningCompiler.

To ensure that everything is working properly you can run our test case that
compare to the reference given by the teacher. you can do so by typing :

42sh$ make test

To clean binaries & trash files generated by the project, you can simply type:

42sh$ make clean


THE COMPILER :


usage :
42sh$ ./ref/osx/TextMiningCompiler /path/to/word/freq.txt /path/to/output/dict.bin

The binary will take a text file as first argument and will generate a
dictionary with the name of the second argument as output. The text file
must respect a proper syntax, which is the word, followed by at least one
space and followed by its frequency and a linefeed. this is an example
of input :

42sh$ cat -e example_word.txt
this     705$
was      695$
a        2014$
cool     758$
project  810$
to       69619$
do       5349$


THE REQUEST APPLICATION:


usage :
42sh$ echo "approx 0 example" | ./TextMiningApp /path/to/compiled/dict.bin
[{"word":"example","freq":984528,"distance":0}]

42sh$ echo "approx 0 anotherone" | ./TextMiningApp /path/to/compiled/dict.bin
[{"word":"anotherone","freq":933,"distance":0}]

42sh$ ./TextMiningApp /path/to/compiled/dict.bin < /path/to/file_with_request.txt
...

This binary will take the dictionary compiled by the compiler as first argument
and will read stdin. The input must have the format given above.
the number given is the maximal distance that we are looking for. A distance
of 0 means that we are looking for the exact word. Be careful that greater
is the distance, greater is the time taken to process the request.
The output result is given in JSON format.


QUESTIONS :


1) We use a trie which is used to store every words in the dictionary
and to find quickly the word we are looking for during the search.
We have also optimized this trie to a radix tree which is basically
a space-optimized trie that remove the internal nodes with only one child which
are not terminal, which means this node is not the last letter of a word.
Every nodes contains as well several metadata such as its numbers
of children or the frequency of the word if this is its last letter.
A radix tree also increase search efficiency as it remove several nodes
and memory usage.

For the request search, we use an optimized way to go through the radix tree
with Levenshtein distance. We keep in memory only the two previous rows as
this is the only data we need to go through the tree applying
Damerau-Levenshtein distance. As we chose C programming, we did our best to
keep it simple to get as much performances as we could with a relatively simple
design.

2) As we progressed through the implementation of our spell checker, several
kind of tests were made to ensure that the program is working as expected and
fulfilling the subject's requirement.

The test suite is our main tool to test the software. It ensures that the output
is exactly the one of the reference's binary, comparing the two output. It
compares also the performances and gives a proper ratio between the two,
expressed in requests per seconds.

In addition to the test suite several tests we made individually. For example we
tested the output of the tree, the printing and sorting functions individually,
the software's RAM usage, very high expected distances, and general
comparisons between ref and our spellchecker output.

3) We tried with distances greater than the word's length and the software
doesn't break but seems to print some values that doesn't seems reliable. Our
software is case sensitive and doesn't handle this kind of errors (which
means "Word" will not propose "word" with a distance of 1).

4) As mentioned before, we chose to implement a radix tree as data structure.
This was the best compromise between binary size (which revealed to
be quite acceptable) and the information that we get during the search.
It is juste a trie that is compressed with nodes that can handle more than
one letter to remove useless internal nodes that are not terminals.

We chose this data structure because we saw few online Levenshtein
implementations that were using a trie and were getting great results.
We simply adapted this to a Damerau-levenshtein implementation and
a radix tree instead of a simple trie.

5) In the first time a dummy approach could be to test for a distance of 1 first
, then for a distance of 2, etc... In a dynamic program it could be an
efficient solution because we can send batch of solutions.
If the program is not dynamic we can approximate the right distance by looking
at the distance of letters in a keyboard layout and established a score for each
couple of consecutive letters.

6) Boosting our program performance has been a huge factor in our choice of
implementation. First, we chose the radix tree and a damerau-levenshtein
specific to trie. We chose as well C language to keep a good language efficiency.
We even wrote the min3 function that is call a very numerous time in assembly
language to improve performances. We tried as well to remove as much useless
program part that we could to reduce the bottleneck function search_rec as
much as possible.

To improve the distance 0 search we decide to drop the Levenshtein computation
and replace it by a simple search of the word in the Trie. hThis way we can beat
the ref by a factor of 3 on exact search of words.

We chose a special format to save the Trie in a binary format. We decide to
remove non necessary metadatas from structures to keep only the frequency and
the number of children of each node. Then the letters of the node are written
followed by a '\0'. The structure is only 5 bytes.

Structure of TrieNodeCompact:

 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                            Frequency                          |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|  nb_children  |     letter    |     letter   ...      '\0'    |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+...+-+-+-+-+-+-+-+

The resulting binary file size of the given dictionary words.txt is only 27M.

To improve these performances again and reach a level competitive with the
given refs, the only way seems to use state-of-the art algorithm that we
will shortly discuss in the next question.

7) A lot of articles speak about BK-Tree implementation for example (building
a graph depending of the distance between words) to speedup the search but the
compilation time is very time consuming.
Another articles used a reverse search dictionary to search all possible
deletions very fast. The dictionary keep the deleted version of strings as the
keys. The values are the strings before deletion.
For example the word 'test' can be derived to 'est', 'tst', 'tet', 'tes'
(these 4 words have a distance of 1 to 'test').
The dictionary will be :
{
'est' -> 'test'
'tst' -> 'test'
'tet' -> 'test'
'tes' -> 'test'
}
If we try to find words at distance 1 from 'text', we can derived 'text' to
'ext', 'txt', 'tet', 'tex'. The token 'tet' will match in the dictionary so
'text' is at distance 1 from 'test' using deletion.
It is a very efficient technique using a hash table because the cost to look
for a match is constant O(1). The technique is also limited to deletion because
a dictionary with deletions, additions, replacement and swap of letters is too
space and time consuming.
The last type of algorithms are probabilistic algorithm. We could not test them
here because the results must be exact. There are optimized versions of
Damerau-Levenshtein using bitwise operations but they are based on probability
and thus introduce an error coefficient.
